{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b17109-6585-40a9-bf5e-d55041c41e13",
   "metadata": {},
   "source": [
    "# Project: Question-Answering on Private Documents (RAG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27e327fc-06dd-4605-a71d-e2bb1b7a700e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv(), override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc77b16d-c58b-42c7-a26a-1fa04b0ad3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76fa38b5-1732-4d6d-8ce0-b4c3f2c767e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a PDF file using LangChain's PyPDFLoader\n",
    "def load_document(file_path):\n",
    "    from langchain.document_loaders import PyPDFLoader\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    return loader.load()\n",
    "\n",
    "# Function to chunk the document into smaller pieces\n",
    "def chunk_data(data, chunk_size=256, chunk_overlap=20):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.split_documents(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9581c89c-2e5a-49cc-b932-c71bf0d65d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_and_get_answer(vector_store, q, k=3):\n",
    "    from langchain.chains import RetrievalQA\n",
    "    from langchain_openai import ChatOpenAI\n",
    "\n",
    "    llm = ChatOpenAI(model='gpt-3.5-turbo', temperature=1)\n",
    "\n",
    "    retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': k})\n",
    "\n",
    "    chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "    \n",
    "    answer = chain.invoke(q)\n",
    "    return answer\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "617b9d83-216b-45a9-a68f-d2c1944459c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings_chroma(chunks, persist_directory='./chroma_db'):\n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "    # Instantiate an embedding model from OpenAI (smaller version for efficiency)\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536)  \n",
    "\n",
    "    # Create a Chroma vector store using the provided text chunks and embedding model, \n",
    "    # configuring it to save data to the specified directory \n",
    "    vector_store = Chroma.from_documents(chunks, embeddings, persist_directory=persist_directory) \n",
    "\n",
    "    return vector_store  # Return the created vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a7b4e151-60ea-45e4-8b10-1c1c423d4dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings_chroma(persist_directory='./chroma_db'):\n",
    "    from langchain.vectorstores import Chroma\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "    # Instantiate the same embedding model used during creation\n",
    "    embeddings = OpenAIEmbeddings(model='text-embedding-3-small', dimensions=1536) \n",
    "\n",
    "    # Load a Chroma vector store from the specified directory, using the provided embedding function\n",
    "    vector_store = Chroma(persist_directory=persist_directory, embedding_function=embeddings) \n",
    "\n",
    "    return vector_store  # Return the loaded vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b7675f2-85d6-4c2a-82cd-c3cecb882928",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the pdf document into LangChain \n",
    "data = load_document('psychologyMoney.pdf')\n",
    "\n",
    "# Splitting the document into chunks\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "\n",
    "# Creating a Chroma vector store using the provided text chunks and embedding model (default is text-embedding-3-small)\n",
    "vector_store = create_embeddings_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1084f7f2-38ed-4a67-b664-0d85726ae485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'what is Milanković’s theory?', 'result': \"Milanković’s theory proposes that variations in Earth's orbit and tilt cause changes in the intensity of sunlight received at different latitudes and times of the year, leading to changes in climate and the onset of ice ages. Initially, it was believed that these changes caused extremely cold winters, but later research by Wladimir Köppen suggested that moderately cool summers were actually the key factor in the freezing process.\"}\n"
     ]
    }
   ],
   "source": [
    "# Asking questions\n",
    "q = 'what is Milanković’s theory?'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "71fc3023-eae6-4ee2-8a94-2bfcb6ae0b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Milanković’s theory proposes that variations in Earth's orbit and tilt cause changes in the intensity of sunlight received at different latitudes and times of the year, leading to changes in climate and the onset of ice ages. Initially, it was believed that these changes caused extremely cold winters, but later research by Wladimir Köppen suggested that moderately cool summers were actually the key factor in the freezing process.\n"
     ]
    }
   ],
   "source": [
    "print(answer['result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "482b084c-e74a-4fbd-ac06-f951518237a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'query': 'Derek Thompson of The Atlantic says what?', 'result': 'This quote from Derek Thompson of The Atlantic highlights how portable devices have made the modern factory location-independent, as work can be done anywhere, turning the entire day into a productive period.'}\n"
     ]
    }
   ],
   "source": [
    "# Load a Chroma vector store from the specified directory (default ./chroma_db) \n",
    "db = load_embeddings_chroma()\n",
    "q = 'Derek Thompson of The Atlantic says what?'\n",
    "answer = ask_and_get_answer(vector_store, q)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95acf7ef-9bb3-43f1-9b2d-9b19e39341aa",
   "metadata": {},
   "source": [
    "## Adding Memory (Chat History)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "55f37708-f5e0-4f5e-923c-c06d87c21d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: People often misjudge the impact of compounding over time because they are accustomed to how quickly things can grow. This familiarity with rapid growth can lead individuals to overlook the power of compounding, causing them to focus on solving problems through other means instead.\n",
      "Source documents: 0\n",
      "\n",
      "Second Answer: One specific example of why people often misjudge the impact of compounding over time is the concept of exponential growth. When investments or savings grow exponentially over time due to compounding, the growth rate accelerates as the base amount increases. This means that even small contributions or returns can lead to significant wealth accumulation over long periods. However, because exponential growth is not intuitive for many people, they may underestimate the long-term impact of compounding and delay starting to save or invest.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ChatMessageHistory, ConversationBufferMemory\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Loading vector store (assumes it's already created or loaded)\n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
    "\n",
    "# Create memory object for the chat history\n",
    "history = ChatMessageHistory()\n",
    "memory = ConversationBufferMemory(\n",
    "    chat_memory=history, \n",
    "    memory_key=\"chat_history\",  # ✅ Specify memory key\n",
    "    return_messages=True\n",
    ")\n",
    "\n",
    "# Instantiate the LLM\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "# Create the ConversationalRetrievalChain\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    chain_type='stuff',\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# function that handles chat_history properly\n",
    "def ask_question_v1(q, chain):\n",
    "    # Pass both question and empty chat_history for first question\n",
    "    result = chain.invoke({\n",
    "        'question': q,\n",
    "        'chat_history': []  # Empty for first question, will be managed by memory\n",
    "    })\n",
    "    return result\n",
    "\n",
    "# Alternative function that relies entirely on memory\n",
    "def ask_question_v2(q, chain):\n",
    "    # Since we're using memory, we can just pass the question\n",
    "    # The chain will handle chat_history through the memory component\n",
    "    result = chain.invoke({'question': q})\n",
    "    return result\n",
    "\n",
    "# More robust function with error handling\n",
    "def ask_question_v3(q, chain):\n",
    "    try:\n",
    "        # Try with just the question first (relies on memory)\n",
    "        result = chain.invoke({'question': q})\n",
    "        return result\n",
    "    except ValueError as e:\n",
    "        if \"chat_history\" in str(e):\n",
    "            # If chat_history is required, provide it\n",
    "            result = chain.invoke({\n",
    "                'question': q,\n",
    "                'chat_history': []\n",
    "            })\n",
    "            return result\n",
    "        else:\n",
    "            raise e\n",
    "\n",
    "# Load and process the document\n",
    "data = load_document('psychologyMoney.pdf')\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "vector_store = create_embeddings_chroma(chunks)\n",
    "\n",
    "# Test the question\n",
    "q = 'Why do people often misjudge the impact of compounding over time?'\n",
    "\n",
    "try:\n",
    "    result = ask_question_v1(q, crc)  # Use solution 1\n",
    "    print(\"Answer:\", result['answer'])\n",
    "    print(\"Source documents:\", len(result.get('source_documents', [])))\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# For subsequent questions, the memory will maintain the chat history automatically\n",
    "q2 = \"Can you give me a specific example of this misjudgment?\"\n",
    "try:\n",
    "    result2 = ask_question_v1(q2, crc)\n",
    "    print(\"\\nSecond Answer:\", result2['answer'])\n",
    "except Exception as e:\n",
    "    print(f\"Error on second question: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924280b1-2cb5-4194-93fb-d6fde83f39ae",
   "metadata": {},
   "source": [
    "## Using a Custom Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2d45d659-fb37-4d15-8bfb-97cb72c83fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People often misjudge the impact of compounding over time because they are accustomed to how quickly things can grow in other areas of their lives. In today's fast-paced society, we are used to instant gratification and immediate results. This mindset can lead people to overlook the power of compounding, which involves gradual growth over time.\n",
      "\n",
      "For example, when it comes to investing, individuals may be more inclined to seek out quick gains or high-risk investments that promise immediate returns. They may overlook the potential of compounding interest, which can significantly increase their wealth over the long term. Similarly, in personal finance, people may prioritize paying off debt quickly rather than taking advantage of the benefits of compounding by investing or saving for the future.\n",
      "\n",
      "Additionally, the concept of compounding may not be intuitive to everyone, leading them to underestimate its impact. It can be challenging for individuals to grasp the idea that small, consistent contributions or gains can accumulate and grow exponentially over time. This lack of understanding can cause people to overlook the potential of compounding and focus on solving financial problems through other means.\n",
      "\n",
      "In conclusion, people often misjudge the impact of compounding over time due to their familiarity with quick growth in other aspects of life, a preference for immediate results, and a lack of intuitive understanding of how compounding works. By recognizing the power of compounding and incorporating it into their financial decisions, individuals can set themselves up for long-term financial success.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Setup\n",
    "retriever = vector_store.as_retriever(search_type='similarity', search_kwargs={'k': 5})\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "llm = ChatOpenAI(model_name='gpt-3.5-turbo', temperature=0)\n",
    "\n",
    "# Custom prompt template\n",
    "custom_template = \"\"\"You are a financial psychology expert. Answer the question using the context provided.\n",
    "\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "\n",
    "Provide a clear, detailed answer with practical examples:\"\"\"\n",
    "\n",
    "custom_prompt = PromptTemplate(\n",
    "    template=custom_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create chain with custom prompt\n",
    "crc = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": custom_prompt}\n",
    ")\n",
    "\n",
    "# Ask questions\n",
    "def ask_question(question, chain):\n",
    "    result = chain.invoke({'question': question, 'chat_history': []})\n",
    "    return result['answer']\n",
    "\n",
    "# Usage\n",
    "data = load_document('psychologyMoney.pdf')\n",
    "chunks = chunk_data(data, chunk_size=256)\n",
    "vector_store = create_embeddings_chroma(chunks)\n",
    "\n",
    "q = 'Why do people often misjudge the impact of compounding over time?'\n",
    "answer = ask_question(q, crc)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0543692c-acae-41a3-81a8-6d5123e31d8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
